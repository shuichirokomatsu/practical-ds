{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "setup"
   },
   "outputs": [],
   "source": [
    "source(here::here(\"R/setup.R\"))\n",
    "library(textrecipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストデータの取り扱い\n",
    "\n",
    "\n",
    "カテゴリを参照\n",
    "\n",
    "カテゴリより長く、基本的にユニークな値をもつものを扱う\n",
    "\n",
    "周辺の土地利用の状況が記載されたs`surrounding_present_usage`を例に示します。 \n",
    "\n",
    "```{r}\n",
    "na.omit(df_lp_kanto$surrounding_present_usage)[seq_len(10)]\n",
    "```\n",
    "\n",
    "```{r}\n",
    "df_lp_surrounding_present_usage_count <- \n",
    "  df_lp_kanto %>% \n",
    "  count(surrounding_present_usage, sort = TRUE)\n",
    "\n",
    "nrow(df_lp_surrounding_present_usage_count)\n",
    "\n",
    "df_lp_surrounding_present_usage_count\n",
    "```\n",
    "\n",
    "```{r}\n",
    "df_lp_surrounding_present_usage_count %>% \n",
    "  ggplot(aes(n)) +\n",
    "  geom_density()\n",
    "```\n",
    "\n",
    "\n",
    "## テキストの前処理\n",
    "\n",
    "### ユニコード正規化\n",
    "\n",
    "```{r}\n",
    "df_lp_kanto %>% \n",
    "  filter(str_detect(surrounding_present_usage, \"ＩＣ\")) %>% \n",
    "  pull(surrounding_present_usage)\n",
    "\n",
    "df_lp_prep <- \n",
    "  df_lp_kanto %>% \n",
    "  recipe(~ .) %>% \n",
    "  step_stri_trans(surrounding_present_usage, trans_id = \"nfkc\") %>% \n",
    "  prep(strings_as_factors = FALSE) %>% \n",
    "  juice()\n",
    "\n",
    "df_lp_prep %>% \n",
    "  filter(str_detect(surrounding_present_usage, \"IC\")) %>% \n",
    "  pull(surrounding_present_usage)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "語順を考慮する・しない\n",
    "\n",
    "考慮しない... 文章の内容を分類\n",
    "考慮する... n-gram (感情分析)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bag-of-Words\n",
    "\n",
    "* テキスト文書を単語の出現回数のベクトルで表現。\n",
    "    * 単語がテキストに現れない場合、対応する要素の値は0になる\n",
    "    * 単語の並び、階層の概念を表現しない。Bag-of-wordsでこれらの意味はない\n",
    "        * →テキストの意味を正しく理解したい場合にはあまり役立たない\n",
    "            * →Bag-of-n-Grams\n",
    "* 全ての単語を同じようにしてカウントすると、必要以上に強調される単語が出る\n",
    "    * 単純な出現頻度だけでは文書の特徴を表現できない\n",
    "    * 「意味のある」単語が強調されるような特徴を表現する方法を用いるべき\n",
    "        * →TF-IDF… TFとIDFの積\n",
    "\n",
    "\n",
    "トークン化\n",
    "\n",
    "```{r}\n",
    "\n",
    "library(tokenizers)\n",
    "tokenizers::tokenize_words(as.character(d$surrounding_present_usage[1]))\n",
    "\n",
    "library(textrecipes)\n",
    "library(recipes)\n",
    "\n",
    "data(okc_text)\n",
    "\n",
    "okc_rec <- recipe(~ ., data = okc_text) %>%\n",
    "  step_tokenize(essay0) %>%\n",
    "  step_tokenfilter(essay0, min_times = 10) \n",
    "\n",
    "okc_obj <- okc_rec %>%\n",
    "  prep(training = okc_text, retain = TRUE)\n",
    "\n",
    "juice(okc_obj) %>% \n",
    "  slice(2) %>% \n",
    "  pull(essay0)\n",
    "\n",
    "\n",
    "data(okc_text)\n",
    "okc_text %>% head()\n",
    "\n",
    "okc_rec <- \n",
    "  recipe(~ ., data = okc_text) %>%\n",
    "  step_tokenize(essay0)\n",
    "\n",
    "# step_tokenize()\n",
    "# %>%\n",
    "#   step_tokenfilter(essay0, max_tokens = 10) %>%\n",
    "#   step_texthash(essay0)\n",
    "\n",
    "okc_obj <- \n",
    "  okc_rec %>%\n",
    "  prep(training = okc_text, retain = TRUE)\n",
    "\n",
    "bake(okc_obj, okc_text) %>% \n",
    "  select(starts_with(\"essay0_hash\")) %>% \n",
    "  summarise_all(sd) %>% \n",
    "  tidyr::gather() %>% \n",
    "  arrange(desc(value))\n",
    "```\n",
    "\n",
    "<!-- ハッシュ化、ハッシュ関数自体は categorical?? -->\n",
    "\n",
    "\n",
    "## 単語の除去\n",
    "\n",
    "Bag-of-Wordsでは、対象の変数に含まれる単語を元に特徴量が生成されますが、中には価値のない単語も含まれます。こうした単語をあらかじめ取り除いておくことは、特徴選択におけるフィルタ方の作業に該当します。\n",
    "\n",
    "文章の特徴を反映しないような前置詞や冠詞などはその代表です。データ全体で出現頻度の少ない単語も役立つ可能性は低いです。こうした不要な単語が特徴量に含まれないよう、あらかじめ有用でない単語を除去するという方が取られます。\n",
    "\n",
    "### ストップワードによる単語除去\n",
    "\n",
    "\n",
    "```{r}\n",
    "  step_tokenize(surrounding_present_usage) %>% \n",
    "  step_stopwords(surrounding_present_usage, custom_stopword_source = \"\")\n",
    "```\n",
    "\n",
    "\n",
    "### 出現頻度による単語のフィルタ\n",
    "\n",
    "```{r}\n",
    "library(recipes)\n",
    "data(covers)\n",
    "\n",
    "rec <- recipe(~ description, covers) %>%\n",
    "  step_count(description, pattern = \"(rock|stony)\", result = \"rocks\") %>%\n",
    "  step_count(description, pattern = \"famil\", normalize = TRUE)\n",
    "\n",
    "rec2 <- prep(rec, training = covers)\n",
    "rec2\n",
    "\n",
    "count_values <- bake(rec2, new_data = covers)\n",
    "count_values\n",
    "\n",
    "tidy(rec, number = 1)\n",
    "tidy(rec2, number = 1)\n",
    "```\n",
    "\n",
    "\n",
    "## tf-idf\n",
    "\n",
    "## 文字列のカウント\n",
    "\n",
    "\n",
    "```{r}\n",
    "d <- \n",
    "  df_lp_kanto %>% \n",
    "  sample_n(10) %>% \n",
    "  select(acreage, surrounding_present_usage)\n",
    "\n",
    "recipe(~ ., data = d) %>%\n",
    "  step_tokenize(surrounding_present_usage, token = \"words\") %>% \n",
    "  prep(training = d, retain = TRUE) %>% \n",
    "  juice() %>% \n",
    "  tidyr::unnest(.id = \"id\") %>% \n",
    "  group_by(id, surrounding_present_usage) %>% \n",
    "  mutate(n = n()) %>% \n",
    "  ungroup() %>% \n",
    "  arrange(desc(n))\n",
    "\n",
    "recipe(~ ., data = d) %>% \n",
    "  step_count(surrounding_present_usage, pattern = \"(住宅\", result = \"count_house\") %>% \n",
    "  prep(d) %>% \n",
    "  juice()\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# step_tokenfilter()\n",
    "```\n",
    "\n",
    "## ステミング\n",
    "\n",
    "形態素?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "## 関連項目\n",
    "\n",
    "- 次元削減\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "- Benjamin Bengfort, Tony Ojeda, Rebecca Bilbro (2018). Applied Text Analysis with Python Enabling Language-Aware Data Products with Machine Learning (O'Reilly)\n",
    "- Julia Silge and David Robinson (2017). [Text Mining with R A Tidy Approach](https://www.tidytextmining.com/) (O'Reilly) (**翻訳** 長尾高弘訳 (2018). Rによるテキストマイニング — tidytextを活用したデータ分析と可視化の基礎 (オライリー))\n",
    "- Alice Zheng and Amanda Casari (2018). Feature Engineering for Machine Learning (O'Reilly) (**翻訳** 株式会社ホクソエム訳 (2019). 機械学習のための特徴量エンジニアリング (オライリー))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
