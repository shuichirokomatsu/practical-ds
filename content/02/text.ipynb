{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": true,
    "hide_output": true,
    "name": "setup",
    "results": "\"hide\""
   },
   "outputs": [],
   "source": [
    "source(here::here(\"R/setup.R\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "eval": true,
    "lines_to_next_cell": 2,
    "name": "load_chapter_depends"
   },
   "outputs": [],
   "source": [
    "library(textrecipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストデータの取り扱い\n",
    "\n",
    "\n",
    "カテゴリを参照\n",
    "\n",
    "カテゴリより長く、基本的にユニークな値をもつものを扱う\n",
    "\n",
    "自然言語処理の領域に入ります。\n",
    "\n",
    "地価公示データの中から、周辺の土地利用の状況が記載された`surrounding_present_usage`を例に解説していきます。まずはこの変数にどのような文字情報が含まれているか確認しておきましょう。\n",
    "\n",
    "```{r}\n",
    "na.omit(df_lp_kanto$surrounding_present_usage)[seq_len(10)]\n",
    "```\n",
    "\n",
    "テキストデータを扱ううえで大事になってくるのは、文字の長さや区切り位置の有無、そして言語です。\n",
    "\n",
    "周辺の土地利用の状況は地価公示データのすべてのデータに対して与えられています。ユニークな件数を数えるとおよそデータの半分の数になります。\n",
    "\n",
    "\n",
    "比較的短文で、重複のあるデータですが\n",
    "\n",
    "ここでは我々が普段扱う日本語のテキストデータに対象を制限しますが、時には英語を扱うこともあります。\n",
    "\n",
    "自然言語処理の分野では言語により、扱い方が異なります。\n",
    "\n",
    "\n",
    "また本来テキストデータとして扱う文字数はこれよりも多いはずですが、基本的な処理は共通です。\n",
    "\n",
    "日本語や中国語など、単語境界をもたない言語に対しては形態素解析などの前処理が必要になります。\n",
    "\n",
    "ここでは範囲を超えるので参考資料をみてください。\n",
    "\n",
    "```{r}\n",
    "df_lp_surrounding_present_usage_count <- \n",
    "  df_lp_kanto %>% \n",
    "  count(surrounding_present_usage, sort = TRUE)\n",
    "\n",
    "nrow(df_lp_surrounding_present_usage_count)\n",
    "```\n",
    "\n",
    "```{r, eval = FALSE, echo = TRUE}\n",
    "df_lp_kanto %>% \n",
    "  mutate(nchar = as.character(stringr::str_length(surrounding_present_usage))) %>% \n",
    "  count(nchar) %>% \n",
    "  ggplot(aes(nchar, n)) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  ggtitle(\"「周辺の土地利用」文字数のカウント\") +\n",
    "  xlab(\"字数\") +\n",
    "  ylab(\"データ件数\")\n",
    "```\n",
    "\n",
    "\n",
    "## テキストの前処理\n",
    "\n",
    "テキストデータは、その独自の構造のために数値やカテゴリなどとは異なる前処理が必要になります。具体的には、空白文字列の削除、記号や区切り文字の削除、ユニコード正規化などです。\n",
    "\n",
    "![](../images/text_clearing.png)\n",
    "\n",
    "### 不要な文字列の削除\n",
    "\n",
    "- ハッシュタグ\n",
    "- URL\n",
    "\n",
    "特定の文字列に変換するという手法も用いられます。対象の文字列を別のものに変える、すなわち置換の処理は正規表現によるパターンマッチを通して行われるのが一般的です。\n",
    "\n",
    "### ユニコード正規化\n",
    "\n",
    "大文字小文字の区別\n",
    "\n",
    "```{r}\n",
    "df_lp_kanto %>% \n",
    "  filter(str_detect(surrounding_present_usage, \"ＩＣ\")) %>% \n",
    "  pull(surrounding_present_usage)\n",
    "\n",
    "df_lp_prep <- \n",
    "  df_lp_kanto %>% \n",
    "  recipe(~ .) %>% \n",
    "  step_stri_trans(surrounding_present_usage, trans_id = \"nfkc\") %>% \n",
    "  prep(strings_as_factors = FALSE) %>% \n",
    "  juice()\n",
    "\n",
    "df_lp_prep %>% \n",
    "  filter(str_detect(surrounding_present_usage, \"IC\")) %>% \n",
    "  pull(surrounding_present_usage)\n",
    "```\n",
    "\n",
    "\n",
    "## Bag-of-Words\n",
    "\n",
    "* テキスト文書を単語の出現回数のベクトルで表現。\n",
    "    * 単語がテキストに現れない場合、対応する要素の値は0になる\n",
    "    * 単語の並び、階層の概念を表現しない。Bag-of-wordsでこれらの意味はない\n",
    "        * →テキストの意味を正しく理解したい場合にはあまり役立たない\n",
    "            * Bag-of-n-Grams\n",
    "* 全ての単語を同じようにしてカウントすると、必要以上に強調される単語が出る\n",
    "    * 単純な出現頻度だけでは文書の特徴を表現できない\n",
    "    * 「意味のある」単語が強調されるような特徴を表現する方法を用いるべき\n",
    "\n",
    "\n",
    "トークン化\n",
    "\n",
    "```{r}\n",
    "text_rec <- \n",
    "  df_lp_kanto %>% \n",
    "  recipe(~ surrounding_present_usage + .row_id) %>% \n",
    "  step_tokenize(surrounding_present_usage)\n",
    "\n",
    "df_lp_token <- \n",
    "  text_rec %>% \n",
    "  prep() %>% \n",
    "  juice() %>% \n",
    "  tidyr::unnest(cols = c(surrounding_present_usage))\n",
    "\n",
    "df_lp_token %>% \n",
    "  filter(.row_id == \"13534\")\n",
    "```\n",
    "\n",
    "\n",
    "```{r}\n",
    "df_lp_texthash <- \n",
    "  text_rec %>%\n",
    "  step_tokenfilter(surrounding_present_usage, max_tokens = 5) %>%\n",
    "  step_texthash(surrounding_present_usage) %>% \n",
    "  prep(retain = TRUE) %>% \n",
    "  bake(df_lp_token)\n",
    " \n",
    "\n",
    "\n",
    "df_lp_texthash %>% \n",
    "  select(starts_with(\"surrounding_present_usage_hash\")) %>% \n",
    "  summarise_all(sd) %>% \n",
    "  tidyr::gather() %>% \n",
    "  arrange(desc(value))\n",
    "```\n",
    "\n",
    "語順を考慮する・しない\n",
    "\n",
    "考慮しない... 文章の内容を分類\n",
    "考慮する... n-gram (感情分析)\n",
    "\n",
    "\n",
    "## 単語の除去\n",
    "\n",
    "Bag-of-Wordsでは、対象の変数に含まれる単語を元に特徴量が生成されますが、中には価値のない単語も含まれます。こうした単語をあらかじめ取り除いておくことは、特徴選択におけるフィルタ法の作業に相当します。\n",
    "\n",
    "文章の特徴を反映しないような前置詞や冠詞などはその代表です。データ全体で出現頻度の少ない単語も役立つ可能性は低いです。こうした不要な単語が特徴量に含まれないよう、あらかじめ有用でない単語を除去するという方法が取られます。\n",
    "\n",
    "\n",
    "### ストップワードによる単語除去\n",
    "\n",
    "文書の内容に重要でないと考えられる単語をまとめて対象から除外する処理にストップワードが用いられます。ここでの重要でない、は文書中に頻繁に出現する句読点や「です」「ます」などの単語などです。ストップワードの選別には、言語に固有のリスト[^1]を使うか、データセットに応じた頻度の高い単語のリストを作成するか、という選択肢があります。\n",
    "\n",
    "```{r, eval = TRUE}\n",
    "df_lp_kanto %>% \n",
    "  recipe(~ .) %>%\n",
    "  step_rm(-acreage, -surrounding_present_usage) %>% \n",
    "  step_tokenize(surrounding_present_usage) %>% \n",
    "  step_stopwords(surrounding_present_usage, custom_stopword_source = \n",
    "                   c(\"アパート\", \"住宅\", \"等\", \"の\", \"が\")) %>% \n",
    "  prep(training = df_lp_kanto) %>% \n",
    "  juice() %>% \n",
    "  tidyr::unnest(cols = c(surrounding_present_usage)) %>% \n",
    "  count(surrounding_present_usage, sort = TRUE)\n",
    "```\n",
    "\n",
    "### 出現頻度による単語のフィルタ\n",
    "\n",
    "```{r}\n",
    "text_rec %>%\n",
    "  step_tokenfilter(surrounding_present_usage, min_times = 50) %>% \n",
    "  prep() %>% \n",
    "  juice() %>% \n",
    "  head(1) %>% \n",
    "  tidyr::unnest(cols = c(surrounding_present_usage))\n",
    "```\n",
    "\n",
    "\n",
    "## tf-idf\n",
    "\n",
    "TF-IDF TF()とIDF()の積\n",
    "\n",
    "## 文字列のカウント\n",
    "\n",
    "```{r}\n",
    "d <- \n",
    "  df_lp_kanto %>% \n",
    "  sample_n(10) %>% \n",
    "  select(.row_id, surrounding_present_usage)\n",
    "\n",
    "recipe(~ ., data = d) %>%\n",
    "  step_tokenize(surrounding_present_usage, token = \"words\") %>% \n",
    "  prep(training = d, retain = TRUE) %>% \n",
    "  juice() %>% \n",
    "  tidyr::unnest(cols = c(surrounding_present_usage)) %>% \n",
    "  group_by(.row_id, surrounding_present_usage) %>% \n",
    "  mutate(n = n()) %>% \n",
    "  ungroup() %>% \n",
    "  arrange(desc(n))\n",
    "\n",
    "recipe(~ ., data = d) %>% \n",
    "  step_count(surrounding_present_usage, pattern = \"住宅\", result = \"count_house\") %>% \n",
    "  prep(d) %>% \n",
    "  juice()\n",
    "```\n",
    "\n",
    "```{r}\n",
    "# step_tokenfilter()\n",
    "```\n",
    "\n",
    "## ステミング\n",
    "\n",
    "形態素?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "## 関連項目\n",
    "\n",
    "- [次元削減](../03/dimension-reduction)\n",
    "\n",
    "## 参考文献\n",
    "\n",
    "- Sarah Guido and Andreas Müller (2016). Introduction to Machine Learning with Python A Guide for Data Scientists (O'Reilly) (**翻訳** 中田秀基訳 (2017). Pythonではじめる機械学習\n",
    "- 石田基広 (2017). Rによるテキストマイニング入門 第2版 (森北出版)\n",
    "- Julia Silge and David Robinson (2017). [Text Mining with R A Tidy Approach](https://www.tidytextmining.com/) (O'Reilly) (**翻訳** 長尾高弘訳 (2018). Rによるテキストマイニング — tidytextを活用したデータ分析と可視化の基礎 (オライリー))\n",
    "- Benjamin Bengfort, Tony Ojeda, Rebecca Bilbro (2018). Applied Text Analysis with Python Enabling Language-Aware Data Products with Machine Learning (O'Reilly)\n",
    "- Alice Zheng and Amanda Casari (2018). Feature Engineering for Machine Learning (O'Reilly) (**翻訳** 株式会社ホクソエム訳 (2019). 機械学習のための特徴量エンジニアリング (オライリー))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
